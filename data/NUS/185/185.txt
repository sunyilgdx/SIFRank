Tactons: Structured Tactile Messages for Non-Visual Information Display
Abstract
Tactile displays are now becoming available in a form 
that can be easily used in a user interface. This paper describes
a new form of tactile output. Tactons, or tactile 
icons, are structured, abstract messages that can be used 
to communicate messages non-visually. A range of different
parameters can be used for Tacton construction including
: frequency, amplitude and duration of a tactile 
pulse, plus other parameters such as rhythm and location. 
Tactons have the potential to improve interaction in a 
range of different areas, particularly where the visual display
is overloaded, limited in size or not available, such as 
interfaces for blind people or in mobile and wearable devices
.
.
This paper describes Tactons, the parameters used
to construct them and some possible ways to design them. 
Examples of where Tactons might prove useful in user 
interfaces are given.
Introduction
The area of haptic (touch-based) human computer interaction
(HCI) has grown rapidly over the last few years. A 
range of new applications has become possible now that 
touch can be used as an interaction technique (Wall et al., 
2002). However, most current haptic devices have scant 
provision for tactile stimulation, being primarily pro-grammable
, constrained motion force-feedback devices 
for kinaesthetic display. The cutaneous (skin-based) 
component is ignored even though it is a key part of our 
experience of touch (van Erp, 2002). It is, for example, 
important for recognising texture, and detecting slip, 
compliance and direction of edges. As Tan (1997) says 
"In the general area of human-computer interfaces ... the 
tactual sense is still underutilised compared with vision 
and audition". One reason for this is that, until recently, 
the technology for tactile displays was limited.
Tactile displays are not new but they have not received 
much attention from HCI researchers as they are often 
engineering prototypes or designed for very specific ap
Copyright © 2004, Australian Computer Society, Inc.  This 
paper appeared at the 5th Australasian User Interface Conference
(AUIC2004), Dunedin.  Conferences in Research and Practice
in Information Technology, Vol. 28. A. Cockburn, Ed. Reproduction
for academic, not-for profit purposes permitted provided
this text is included.
plications (Kaczmarek et al., 1991). They have been used 
in areas such as tele-operation or displays for blind people
to provide sensory substitution ­ where one sense is 
used to receive information normally received by another 
(Kaczmarek et al.). Most of the development of these 
devices has taken place in robotics or engineering labs 
and has focused on the challenges inherent in building 
low cost, high-resolution devices with realistic size, 
power and safety performance. Little research has gone 
into how they might actually be used at the user interface. 
Devices are now available that allow the use of tactile 
displays so the time is right to think about how they 
might be used to improve interaction.
In this paper the concept of Tactons, or tactile icons, is 
introduced as a new communication method to complement
graphical and auditory feedback at the user interface
. Tactons are structured, abstract messages that can be 
used to communicate messages non-visually. Conveying 
structured messages through touch will be very useful in 
areas such as wearable computing where screens are limited
. The paper gives some background to the perception  
and use of tactile stimuli and then describes the design of 
Tactons. It finishes with examples of potential uses for 
Tactons.
Background and previous work
The skin is the largest organ in the body, about 2 m
2

in
the average male (Montagu, 1971). Little direct use is 
made of it for displaying information in human-computer 
interfaces (Tan and Pentland, 1997, van Erp, 2002), yet a 
touch on the hand or other parts of the body is a very rich 
experience. The skin can therefore potentially be used as 
a medium to communicate information. As a receiving 
instrument the skin combines important aspects of the eye 
and the ear, with high acuity in both space and time 
(Gunther, 2001) giving it good potential as a communication
medium.
The human sense of touch can be roughly split in to two 
parts: kinaesthetic and cutaneous. "Kinaesthetic" is often 
used as catch-all term to describe the information arising 
from forces and positions sensed by the muscles and 
joints. Force-feedback haptic devices (such as the 
PHANToM from SensAble) are used to present information
to the kinaesthetic sense. Cutaneous perception refers 
to the mechanoreceptors contained within the skin, and 
includes the sensations of vibration, temperature, pain 
and indentation. Tactile devices are used to present feedback
to the cutaneous sense.
15
Current haptic devices use force-feedback to present kinaesthetic
stimuli. This works well for some aspects of 
touch (e.g. identifying the geometric properties of objects
) but is poor for features such as texture (normally 
perceived cutaneously). Oakley et al. (2000) found that 
trying to use texture in a user interface with a force-feedback
device actually reduced user performance. One 
reason for this is that the textures had to be made large so 
that they could be perceived kinaesthetically, but they 
then perturbed users' movements. The use of a tactile 
haptic device to present texture would not have this problem
as small indentations in the fingertip would not affect 
hand movements. At present, however, there are no haptic
devices that do a good job of presenting both tactile 
and force-feedback cues to users.
Current force-feedback devices use a point interaction 
model; the user is represented by a single point of contact 
corresponding to the tip of a stylus. This is analogous to 
exploring the world by remote contact through a stick 
thus depriving the user of the rich, spatially varying cutaneous
cues that arise on the finger pad when contacting a 
real object (Wall and Harwin, 2001). Users must integrate 
temporally varying cues as they traverse the structure of 
virtual objects with the single point of contact, which 
places considerable demands on short-term memory 
(Jansson and Larsson, 2002). Even when exploring simple
geometric primitives, performance is greatly reduced 
compared to natural touch. Lederman and Klatzky (1999) 
have shown that such removal of cutaneous input to the 
fingertip impedes perception of edge direction, which is 
an essential component of understanding haptic objects. It 
can therefore be seen that tactile feedback and cutaneous 
perception are key parts of touch that must be incorporated
into haptic displays if they are to be effective and 
usable.
2.1  Vibrotactile actuators
There are two basic types of vibrotactile display device. 
These evoke tactile sensations using mechanical vibration 
of the skin (usually in the range 10-500Hz) (Kaczmarek 
et al., 1991). This is commonly done by vibrating a small 
plate pressed against the skin or via a pin or array of pins 
on the fingertip. These are very easy to control from standard
PC hardware. Other types of actuator technology  
are available, including pneumatic and electrotactile 
(Stone, 2000), but these tend to be bulkier and harder to 
control so are less useful in many situations.

Figure 1: The pins arrays on the VirTouch tactile
mouse (www.virtouch.com).

The first type of vibrotactile display uses a pin or array of 
small pins (e.g. the VirTouch mouse in Figure 1 or those 
produced by Summers et al. (2001)) to stimulate the fingertip
. Such devices can present very fine cues for surface 
texture, edges, lines, etc. The second type uses larger 
point-contact stimulators (e.g. Figure 2 or alternatively 
small loudspeaker cones playing tones, or other simple 
vibrating actuators placed against the skin as used by Tan 
(1997) and in devices such as the CyberTouch glove ­ 
www.immersion.com). The cues here are much lower 
resolution but can exert more force; they can also be distributed
over the body to allow multiple simultaneous 
cues (often mounted in a vest on the user's back or in a 
belt around the waist). These devices are both easy to 
control and use. For a full review see Kaczmarek et al. 
(1991).

Figure 2: Audiological Engineering Corp. VBW32
transducers (www.tactaid.com).
2.2    Previous work on tactile display
One common form of tactile output is Braille, and dynamic
Braille cells are available. A display is made up of 
a line of `soft' cells (often 40 or 80), each with 6 or 8 pins 
that move up and down to represent the dots of a Braille 
cell. The user can read a line of Braille cells by touching 
the pins of each cell as they pop up (for more information 
see www.tiresias.org). The focus of the work reported 
here is not on Braille as it tends to be used mainly for 
representing text (although other notations are used, e.g. 
music) and the cells are very low resolution (8 pins 
maximum). These displays are also very expensive with 
an 80 cell display costing around £4000. There have been 
many other tactile devices for blind people, such as the 
Optacon (TeleSensory Inc.), which used an array of 144 
pins to display the input from a camera to the fingertip, 
but again these are mainly used for reading text. Pin arrays
produce Braille but can do much more, especially the 
higher resolution displays such as shown in Figure 1.
Our research also builds on the work that has been done 
on tactile graphics for blind people (this mainly takes the 
form of raised lines and dots on special `swell' paper). 
Kurze (1997, 1998) and Challis (2001) have developed 
guidelines which allow images and objects to be presented
that are understandable through touch by blind 
users.
Two other examples show that the cutaneous sense is 
very effective for communication. Firstly, Tadoma is a 
tactile language used by deaf/blind people. The transmitter
speaks normally and the receiver puts a hand on the 
face of the speaker, covering the mouth and neck (Tan 
and Pentland, 2001). Tadoma users can listen at very high
16
speeds (normal speaking speed for experts) and pick up 
subtleties of the speech such as accent. In the second example
, Geldard (1957) taught participants a simple tactile 
language of 45 symbols, using three intensities, three 
durations and five locations on the chest. Participants 
were able to learn the alphabet quickly and could recognise
up to 38 words per minute in some cases. Other sensory
substitution systems convert sound into vibration for 
hearing-impaired people (e.g. the TactAid system from 
Audiological Engineering). Again this shows that cutaneous
perception is very powerful and if we can make use 
of it at the user interfaces we will have a rich new way to 
present information to users.
Research and existing applications have shown that the 
cutaneous sense is a very powerful method of receiving 
information. Other work has shown that it can be used in 
user interfaces and wearable computers (Gemperle et al., 
1998). Tan has begun to investigate the use of tactile displays
on wearable computers (Tan and Pentland, 1997). 
She used a 3x3 grid of stimulators on a user's back to 
provide navigation information. Informal results suggested
it was useful but no formal evaluation has taken 
place. Other relevant work has taken place in aircraft 
cockpits to provide pilots with navigation information 
(van Veen and van Erp, 2001, Rupert, 2000). In these 
examples only simple tactile cues for direction have been 
provided. For example, an actuator maybe vibrated on 
one side of the body to indicate the direction to turn. 
More sophisticated cues could be used to provide much 
more information to users without them needing to use 
their eyes.
Gunther et al. have used tactile cues to present `musical' 
compositions to users (Gunther, 2001, Gunther et al., 
2002). They say: "The approach taken ... views haptic 
technologies ­ in particular the vibrotactile stimulator ­ 
as independent output devices to be used in conjunction 
with the composition and perception of music. Vibrotactile
stimuli are viewed not as signals carrying information 
per se, but as aesthetic artifacts themselves".  He used an 
array of 13 transducers across the body of a `listener' so 
that he/she could experience the combined sonic/tactile 
presentation. Gunther created a series of compositions 
played to listeners who appeared to enjoy them. This 
work was artistic in nature so no formal usability assessments
were made but the listeners all liked the experience
.
In order to create a tactile composition (the same is true 
for the Tactons described below) a good understanding of 
the experience of touch is needed. However, as Gunther 
et al. suggest: "It is indeed premature to hammer out the 
details of a language for tactile composition. It seems 
more productive at this point in time to identify the underpinnings
of such a language, specifically those dimensions
of tactile stimuli that can be manipulated to form 
the basic vocabulary elements of a compositional lan-guage"
. Research is needed to gain a more systematic 
understanding of cutaneous perception for use in the 
presentation of such messages.
Enriquez and MacLean (2003) recently proposed `haptic 
icons', which they define as "brief programmed forces 
applied to a user through a haptic interface, with the role
of communicating a simple idea in a manner similar to 
visual or auditory icons". The problem they are trying to 
address is different to that of Tactons, as they say "With 
the introduction of "active" haptic interfaces, a single 
handle ­ e.g. a knob or a joystick ­ can control several 
different and perhaps unrelated functions. These multi-function
controllers can no longer be differentiated from 
one another by position, shape or texture... Active haptic 
icons, or "hapticons", may be able to solve this problem 
by rendering haptically distinct and meaningful sensations
for the different functions". These use one degree-of
-freedom force-feedback devices, rather than tactile 
displays, so encode information very differently to Tactons
. They report the construction of a tool to allow a user 
to create and edit haptic icons. This is early work and 
they do not report results from the use of hapticons in any 
interfaces.  Their results, however, will be directly relevant
to Tactons.
Tactons
Given that the cutaneous sense is rich and a powerful 
communication medium currently little utilised in HCI, 
how can we make effective use of it? One approach is to 
use it to render objects from the real world more realisti-cally
in virtual environments, for example in improving 
the presentation of texture in haptic devices. It could also 
be used to improve targeting in desktop interactions along 
the lines suggested by Oakley et al. (2000). In this paper 
it is suggested that it can additionally be used to present 
structured informational messages to users.
Tactons are structured, abstract messages that can be used 
to communicate complex concepts to users non-visually. 
Shneiderman (1998) defines an icon as "an image, picture 
or symbol representing a concept". Tactons can represent 
complex interface concepts, objects and actions very con-cisely
.  Visual icons and their auditory equivalent earcons 
(Blattner et al., 1989, Brewster et al., 1994) are very 
powerful ways of displaying information but there is currently
no tactile equivalent. In the visual domain there is 
text and its counterpart the icon, the same is true in sound 
with synthetic speech and the earcon. In the tactile domain
there is Braille but it has no `iconic' counterpart. 
Tactons fill this gap. Icons/Earcons/Tactons form a simple
, efficient language to represent concepts at the user 
interface.
Tactons are similar to Braille in the same way that visual 
icons are similar to text, or earcons are similar to synthetic
speech. For example, visual icons can convey complex
information in a very small amount of screen space, 
much smaller than for a textual description. Earcons convey
information in a small amount of time as compared to 
synthetic speech. Tactons can convey information in a 
smaller amount of space and time than Braille. Research 
will also show which form of iconic display is most suitable
for which type of information. Visual icons are good 
for spatial information, earcons for temporal. One property
of Tactons is that they operate both spatially and 
temporally so they can complement both icons and earcons
. Further research is needed to understand how these 
different types of feedback work together.
17
Using speech as an example from the auditory domain: 
presenting information in speech is slow because of its 
serial nature; to assimilate information the user must hear 
a spoken message from beginning to end and many words 
may have to be comprehended before the message can be 
understood. With earcons the messages are shorter and 
therefore more rapidly heard, speeding up interactions. 
The same is true of Tactons when compared to Braille. 
Speech suffers from many of the same problems as 
graphical text in text-based computer systems, as this is 
also a serial medium. Barker & Manji (1989) claim that 
an important limitation of text is its lack of expressive 
capability: It may take many words to describe a fairly 
simple concept. Graphical iconic displays were introduced
that speeded up interactions as users could see a 
picture of the thing they wanted instead of having to read 
its name from a list (Barker and Manji, 1989). In the 
same way, an encoded tactile message may be able to 
communicate its information in fewer symbols. The user 
feels the Tacton then recalls its meaning rather than having
the meaning described in Braille (or speech or text). 
The icon is also (in principle) universal: it means the 
same thing in different languages and the Tacton would 
have similar universality.
Designing with Tactons
Tactons are created by encoding information using the 
parameters of cutaneous perception. The encoding is 
similar to that of earcons in sound (Blattner et al., 1989, 
Brewster et al., 1994) where each of the musical parameters
(e.g. timbre, frequency, amplitude) is varied to encode
information. Similar parameters can be used for 
Tactons (although their relative importance is different). 
As suggested by Blattner, short motifs could be used to 
represent simple objects or actions and these can then be 
combined in different ways to represent more complex 
messages and concepts. As Tactons are abstract the mapping
between the Tacton and what it represents must be 
learned, but work on earcons has shown that learning can 
take place quickly (Brewster, 1998b).
The properties that can be manipulated for Tactons are 
similar to those used in the creation of earcons. The parameters
for manipulation also vary depending on the 
type of transducer used; not all transducers allow all types 
of parameters. The general basic parameters are:
Frequency: A range of frequencies can be used to differentiate
Tactons. The range of 20 ­ 1000 Hz is perceivable 
but maximum sensitivity occurs around 250 Hz (Gunther 
et al., 2002). The number of discrete values that can be 
differentiated is not well understood, but Gill (2003) suggests
that a maximum of nine different levels can be used. 
As in audition, a change in amplitude leads to a change in 
the perception of frequency so this has an impact on the 
use of frequency as a cue. The number of levels of frequency
that can be discriminated also depends on whether 
the cues are presented in a relative or absolute way. Making
relative comparisons between stimuli is much easier 
than absolute identification, which will lead to much 
fewer discriminable values, as shown in the work on earcon
design (Brewster et al., 1994).
Amplitude: Intensity of stimulation can be used to encode 
values to present information to the user. Gunther (2002) 
reports that the intensity range extends to 55 dB above the 
threshold of detection; above this pain occurs. Craig and 
Sherrick (1982) indicate that perception deteriorates 
above 28 dB so this would seem to be a useful maximum. 
Gunther (2001) reports that various values, ranging from 
0.4dB to 3.2dB, have been reported for the just noticeable 
difference (JND) value for intensity. Gill states that that 
no more than four different intensities should be used 
(Gill, 2003). Again the number of useful discriminable 
values will depend on absolute or relative presentation of 
stimuli. Due to the interactions between this and frequency
several researchers have suggested that they be 
combined into a single parameter to simplify design
Waveform: The perception of wave shape is much more 
limited than with the perception of timbre in sound. Users 
can differentiate sine waves and square waves but more 
subtle differences are more difficult (Gunther, 2001). 
This limits the number of different values that can be encoded
and makes this a much less important variable than 
it is in earcon design (where it is one of the key variables
).
Duration: Pulses of different durations can encode information
. Gunther (2001) investigated a range of subjective 
responses to pulses of different durations. He found that 
stimuli lasting less than 0.1 seconds were perceived as 
taps or jabs whereas stimuli of longer duration, when 
combined with gradual attacks and decays, may be perceived
as smoothly flowing tactile phrases. He suggests 
combining duration with alterations in the envelope of a 
vibration, e.g. an abrupt attack feels like a tap against the 
skin, a gradual attack feels like something rising up out of 
the skin.
Rhythm: Building on from duration, groups of pulses of 
different durations can be composed into rhythmic units. 
This is a very powerful cue in both sound and touch. 
Gunther (2001) suggests that differences in duration can 
be used to group events when multiple events occur on 
the same area of skin.
Specific transducer types allow other parameters to be 
used:
Body location: Spatially distributed transducers can encode
information in the position of stimulation across the 
body. The choice of body location for vibrotactile display 
is important, as different locations have different levels of 
sensitivity and spatial acuity. A display may make use of 
several body locations, so that the location can be used as 
another parameter, or can be used to group tactile stimuli.
The fingers are often used for vibrotactile displays because
of their high sensitivity to small amplitudes and 
their high spatial acuity (Craig and Sherrick, 1982). However
, the fingers are often required for other tasks, so 
other body locations may be more suitable. Craig and 
Sherrick suggest the back, thigh and abdomen as other 
suitable body locations. They report that, once subjects 
have been trained in vibrotactile pattern recognition on 
the back, they can almost immediately recognise the same 
patterns when they are presented to the thigh or abdomen. 
This transfer also occurs to some extent when patterns are
18
presented to different fingers after training on one finger, 
but is not so immediate.
Certain body locations are particularly suitable, or particularly
unsuitable, for certain types of vibrotactile displays
. For example, transducers should not be placed on 
or near the head, as this can cause leakage of vibrations 
into the ears, resulting in unwanted sounds (Gunther et 
al., 2002). An example of a suitable body location is in 
Gunther's Skinscape display, where he positions low frequency
transducers on the torso as this is where low frequencies
are felt when loud music is heard.
The method of attaching the transducers to a user's body 
is also important. The pressure of the transducer against 
the body has a significant effect on the user's perception 
of the vibrations. Transducers should rest lightly on the 
skin, allowing the user to feel the vibration against the 
skin, and to isolate the location of the vibration with ease. 
Exerting too much pressure with the transducer against 
the user's body will cause the vibrations to be felt in the 
bone structure, making them less isolated due to skeletal 
conduction. In addition, tightening the straps holding the 
transducer to achieve this level of pressure may impede 
circulation (Gunther, 2001).
Rupert (2000) suggests using the full torso for displaying 
3D information, with 128 transducers distributed over the 
body. His system displays information to pilots about the 
location of objects around them in 3D space, by stimulating
the transducers at the part of their body corresponding 
to the location of the object in 3D space around them. 
This could be used to indicate horizons, borders, targets, 
or other aircraft.
Spatiotemporal patterns: Related to position and rhythm, 
spatial patterns can also be "drawn" on the user's body. 
For example, if a user has a 3x3 array of stimulators lo-cated
on his/her back, lines and geometric shapes can be 
"drawn" on the back, by stimulating, in turn, the stimulators
that make up that shape. In Figure 3, an `L' shaped 
gesture can be drawn by activating the stimulators: 1-4-78
-9 in turn. Patterns can move about the body, varying in 
time and location to encode information. Cholewiak 
(1996) and Sherrick (1985) have also looked at low-level 
perception of distributed tactile cues.
.

Figure 3: "Drawing" an L-shaped gesture.
Now that the basic parameters for Tactons have been described
, we will give some examples of how they might 
be designed to convey information. The fundamental design
of Tactons is similar to that of earcons.
4.1  Compound Tactons
A simple set of Tactons could be created as in Figure 4. A 
high-frequency pulse that increases in intensity could 
represent `Create', a lower frequency pulse that decreases 
in intensity could represent `Delete'. A two note falling 
Tacton could represent a file and a two rising notes a 
folder. The mapping is abstract; there is no intuitive link 
between what the user feels and what it represents.


Create
Delete
File
Folder
Create File
Delete Folder

Figure 4: Compound Tactons (after Blattner et al.,
1989).
These Tactons can then be combined to create compound 
messages. For example, `create file' or `delete folder'. 
The set of basic elements could be extended and a simple 
language of tactile elements created to provide feedback 
in a user interface.
4.2  Hierarchical Tactons
Tactons could also be combined in a hierarchical way, as 
shown in Figure 5. Each Tacton is a node in a tree and 
inherits properties from the levels above it. Figure 5 
shows a hierarchy of Tactons representing a hypothetical 
family of errors. The top of the tree is a family Tacton 
which has a basic rhythm played using a sinewave (a different
family of errors would use a different rhythm so 
that they are not confused). The rhythmic structure of 
Level 2 inherits the Tacton from Level 1 and adds to it. In 
this case a second, higher frequency Tacton played with a 
squarewave. At Level 3 the tempo of the two Tactons is 
changed. In this way a hierarchical structure can be presented
. The other parameters discussed above could be 
used to add further levels.
4.3  Transformational Tactons
A third type of Tacton is the Transformational Tacton. 
These have several properties, each represented by a different
tactile parameter. For example, if Transformational 
Tactons were used to represent files in a computer interface
, the file type could be represented by rhythm, size by 
frequency, and creation date by body location. Each file 
type would be mapped to a unique rhythm. Therefore, 
two files of the same type, and same size, but different 
creation date would share the same rhythm and frequency
, but would be presented to a different body location
. If two files were of different types but the same size 
they would be represented by different rhythms with the 
same frequency.
19
Uses for Tactons
We are interested in three areas of use for Tactons, although
there are many others where they have potential to 
improve usability.
5.1  Enhancements of desktop interfaces
The first, and simplest, area of interest is in the addition 
of Tactons to desktop graphical interfaces. The addition 
of earcons to desktops has shown many advantages in 
terms of reduced errors, reduced times to complete tasks 
and lowered workload (Brewster, 1998a). One problem 
with audio is that users believe that it may be annoying to 
use (although no research has actually shown this to be 
the case) and it has the potential to annoy others nearby 
(for a discussion see (Brewster, 2002)). The addition of 
Tactons to widgets has the same potential to indicate usability
problems but without the potential to annoy.
One reason for enhancing standard desktop interfaces is 
that users can become overloaded with visual information 
on large, high-resolution displays. In highly complex 
graphical displays users must concentrate on one part of 
the display to perceive the visual feedback, so that feedback
from another part may be missed. This becomes 
very important in situations where users must notice and 
deal with large amounts of dynamic data or output from 
multiple applications or tasks. If information about secondary
tasks was presented through touch then users 
could concentrate their visual attention on the primary 
one but feel information about the others.
As a simple example, the display of a progress bar widget 
could be presented tactually. Two sets of tactile pulses 
could be used to indicate the current and end points of a 
download. The time between the two pulses would indicate
the amount of time remaining, the closer the two 
pulses the nearer the download is to finishing. The two 
pulses could use different waveforms to ensure they were 
not confused. Different rhythms for each pulse could be 
used to indicate different types of downloads. If a more 
sophisticated set of transducers on a belt around the waist 
was available then the position of a pulse moving around 
the body in a clockwise direction (starting from the front) 
would give information about progress: when the pulse 
was at the right side of the body the download would be 
25% of the way through, when it was on the left hand 
side 75%, and when it got back around to the front it 
would be finished. There would be no need for any visual 
presentation of the progress bar, allowing users to focus 
their visual attention on the main task they are involved 
with.
Tactons could also be used to enhance interactions with 
buttons, scrollbars, menus, etc. to indicate when users are 
on targets and when certain types of errors occur. Others 
have shown that basic tactile feedback can improve  
pointing and steering type interactions (Akamatsu et al., 
1995, Campbell et al., 1999). There are some commercial 
systems that give simple tactile feedback in desktop user 
interfaces, e.g. the software that comes with the Logitech 
iFeel mouse (www.logitech.com). This provides basic 
targeting: a brief pulse is played, for example, when a 
user moves over a target. We believe there is much more 
that can be presented with tactile feedback.
5.2  Visually impaired users
Tactons will be able to work alongside Braille in tactile 
displays for blind and visually impaired users, in the same 
way as earcons work alongside synthetic speech. They 
will allow information to be delivered more efficiently. In 
addition, hierarchical Tactons could help users navigate
Sine
Sine
Square
Error
Operating system error
Execution error
Sine
Square
Overflow
Sine
Square
Underflow
Sine
Square
Fast tempo
Slow tempo
Figure 5: Hierarchical Tacton composition.
Level 1
Level 2
Level 3
20
around Braille media by providing navigation information 
(Brewster, 1998b).
One of our main interests is in using Tactons to improve 
access to graphical information non-visually. Text can be 
rendered in a relatively straightforward manner by speech 
or Braille, but graphics are more problematic. One area 
that we and others have focused on is visualisation for 
blind people. Understanding and manipulating information
using visualisations such as graphs, tables, bar charts 
and 3D plots is very common for sighted people. The 
skills needed are learned early in school and then used 
throughout life, for example, in analysing information or 
managing home finances. The basic skills needed for creating
and manipulating graphs are necessary for all parts 
of education and employment. Blind people have very 
restricted access to information presented in these visual 
ways (Edwards, 1995). As Wise et al. (2001) say "Inac-cessibility
of instructional materials, media, and technologies
used in science, engineering, and mathematics 
education severely restricts the ability of students with 
little or no sight to excel in these disciplines". To allow 
blind people to gain the skills needed for the workplace 
new technologies are necessary to make visualisations 
usable. Tactons provide another route through which information
can be presented.
Research has shown that using haptic devices is an effective
way of presenting graphical information non-visually 
(Yu and Brewster, 2003, Wies et al., 2001, Van Scoy et 
al., 2000). The most common approach has been to use 
haptic devices to present graphs, tables or 3D plots that 
users can feel kinaesthetically by tracing a line or shape 
with a finger using a device like the PHANToM 
(www.sensable.com). Lederman and Klatzky  (1999) 
have shown that removal of cutaneous input to the fingertip
impedes perception of edge direction, which is an essential
component of tracing a haptic line graph. This lack 
of cutaneous stimulation leads to problems with navigation
(exploring using a single point of contact means it is 
difficult to locate items as there is no context, which can 
be given in a tactile display), exploring small scale features
(these would be perceived cutaneously on the finger 
pad in real life), and information overload (all haptic information
is perceived kinaesthetically rather than being 
shared with cutaneous perception). Incorporating a tactile 
display into a force-feedback device will alleviate many 
of these problems and potentially increase user efficiency 
and comprehension of visualisations.
Tactons could be presented as the user moves the force-feedback
device over the visualisation. Dimensions of the 
data can be encoded into a Tacton to give information 
about the current point, using the parameters described in 
Section 4. This would allow more data to be presented 
more efficiently. For example, with multidimensional 
data one dimension might be mapped to the frequency of 
a pulse in a Tacton, another might map to rhythm and 
another to body locatoin. As the user moves about the 
data he/she would feel the different parameters. In addition
to the finger pad, we can also include tactile displays 
to other parts of the body (e.g. to the back) using spatially 
distributed transducers to provide even more display area. 
As long as this is done in a comprehensible manner users
will be able to gain access to their data in a much more 
effective way than with current force-feedback only visualisation
tools.
5.3  Mobile and wearable devices
Our other main application area is mobile and wearable 
device displays (for both sighted and blind people). Mobile
telephones and handheld computers are currently one 
of the fastest growth areas of computing and this growth 
will extend into more sophisticated, fully wearable computers
in the future. One problem with these devices is 
their limited output capabilities. Their small displays easily
become cluttered with information and widgets and 
this makes interface design difficult. In addition, users are 
not always looking at the display of a device as they must 
walk or navigate through their environment which requires
visual attention. One way to solve this problem is 
to use other display modalities and so reduce demands on 
visual display, or replace it if not available. Work has 
gone into using speech and non-speech sounds to overcome
the display bottleneck. Tactile displays have great 
potential here too but are much less well investigated.
Sound has many advantages but it can be problematic; in 
loud environments it can be impossible to hear auditory 
output from a device, in quiet places the audio may be 
disturbing to others nearby. Blind people often do not like 
to wear headphones when outdoors as they mask important
environmental sounds. Tactile displays do not suffer 
from these problems (although there may be other problems
for example, perceiving tactile stimuli whilst running
due to the difficulties of keeping the transducers in 
contact with the skin). Mobile telephones commonly have 
a very simple point-contact tactile stimulator built-in that 
can alert the user to a call. These are often only able to 
produce pulses of different durations.  A pin array would 
be possible on such a device as the user will be holding it 
in a hand when in use. Such a sophisticated tactile display 
could do much more, e.g. it could give information on the 
caller, replace or enhance items on the display (like icons, 
progress indicators, games) or aid in the navigation of the 
devices' menus so that the user does not need to look at 
the screen.
In a wearable device users could have body mounted 
transducers so that information can be displayed over 
their body. In the simplest case this could be used to give 
directional information by vibrating one side of the body 
or other to indicate which way to turn (Tan and Pentland, 
1997). A belt of transducers around the waist could give a 
compass-like display of direction; a pulse could be played 
continuously at north so the user can maintain orientation 
after turning (useful when navigating in the dark) or at the 
position around the waist corresponding to the direction 
in which to head. A more sophisticated display might 
give information about the user's context. For example, 
presenting Tactons describing information such as the 
type of building (shop, bank, office-block, house), the 
type of shop (clothes, phones, food, furniture) the price-bracket
of a shop (budget, mid-range, expensive), or information
more related to the concerns of visually impaired
people, such as the number of stairs leading up to 
the entrance (for firefighters, whose vision is impaired
21
due to smoke and flames, a tactile display could also provide
information on the location of rooms and exits in a 
burning building). A tactile display could also present 
information on stock market data (building on from the 
work on tactile visualisation in the section above) so that 
users could keep track of trades whilst away from the 
office. Such tactile displays could also work alongside 
auditory or visual ones.
Future work and conclusions
This paper has laid out some of the foundations of information
display through Tactons. There is still much work 
to be done to fully understand how they should be designed
and used. There are many lower level perceptual 
questions to be addressed before higher level design issues
can be investigated. Many of the parameters of touch 
described in Section 4 are not fully understood and the 
full usable ranges of the parameters are not known. Studies
need to be undertaken to explore the parameter space 
so that the relative importance of the different parameters 
can be discovered.
Once the range of parameters is understood then the construction
of Tactons can be examined. Basic studies are 
needed to understand how the parameters can be combined
to construct Tactons. Parameters which work well 
alone may not work well when combined with others into 
a Tacton. For example, one parameter may mask another. 
When the basic design of Tactons is understood the composition
of simple Tactons into more complex messages, 
encoding hierarchical information into Tactons, and their 
learnability and memorability can be investigated. The 
concurrent presentation of multiple Tactons must also be 
studied. These studies will answer some of the main questions
regarding the usability of Tactons and a good understanding
of their design and usability will have been a-chieved
.
Another important task is to investigate the strong relationships
between hearing and touch by examining cross-modal
uses of audio and tactile multimodal displays 
(Spence and Driver, 1997), e.g. combined audio and tactile
cues, redundant tactile and audio cues, and moving 
from an audio to a tactile presentation of the same information
(and vice versa). This is important in a mo-bile/wearable
context because at different times different 
display techniques might be appropriate. For example, 
audio might be inappropriate in a very noisy environment
, or tactile cues might be masked when the user is 
running. One important issue is to identify the types of 
information best presented in sound and those best presented
tactually. For example, the range of the vibrotactile
frequency response is roughly 20 times less than that 
of the auditory system. Such discrepancies must be accounted
for when performing cross-modal mappings from 
hearing to touch.
In conclusion, this paper has proposed a new form of tactile
output called Tactons. These are structured tactile 
messages that can be used to communicate information. 
Tactile output is underused in current interfaces and Tactons
provide a way of addressing this problem. The basic 
parameters have been described and design issues discussed
. A technique is now available to allow tactile display
to form a significant part of the set of interaction and 
display techniques that can be used to communicate with 
users at the interface.
Acknowledgements
This research was conducted when Brewster was on sabbatical
in the Department of Computer Science at the 
University of Canterbury, Christchurch, New Zealand. 
Thanks to Andy Cockburn for his thoughts and comments 
on this work. The sabbatical was funded by an Erskine 
Fellowship from the University of Canterbury. The work 
was part funded by EPSRC grant GR/S53244. Brown is 
funded by an EPSRC studentship.
References
Akamatsu, M., MacKenzie, I. S. and Hasbrouq, T.
(1995): A comparison of tactile, auditory, and visual 
feedback in a pointing task using a mouse-type device
. Ergonomics, 38, 816-827.
Barker, P. G. and Manji, K. A. (1989): Pictorial dialogue
methods. International Journal of Man-Machine Studies
, 31, 323-347.
Blattner, M., Sumikawa, D. and Greenberg, R. (1989):
Earcons and icons: Their structure and common design
principles. Human Computer Interaction,  4, 11-44
.
Brewster, S. A. (1998a): The design of sonically-enhanced
widgets. Interacting with Computers,  11, 
211-235.
Brewster, S. A. (1998b): Using Non-Speech Sounds to
Provide Navigation Cues. ACM Transactions on 
Computer-Human Interaction, 5, 224-259.
Brewster, S. A. (2002): Chapter 12: Non-speech auditory
output. In The Human Computer Interaction Handbook
(Eds, Jacko, J. and Sears, A.) Lawrence Erlbaum 
Associates, pp. 220-239.
Brewster, S. A., Wright, P. C. and Edwards, A. D. N.
(1994): A detailed investigation into the effectiveness 
of earcons. In Auditory Display (Ed, Kramer, G.) Addison
-Wesley, Reading, MA, pp. 471-498.
Campbell, C., Zhai, S., May, K. and Maglio, P. (1999):
What You Feel Must Be What You See: Adding Tactile
Feedback to the Trackpoint. Proceedings of IFIP 
INTERACT'99, Edinburgh, UK, 383-390, IOS Press
Challis, B. and Edwards, A. D. N. (2001): Design principles
for tactile interaction. In Haptic Human-Computer
Interaction, Vol. 2058 (Eds, Brewster, S. 
A. and Murray-Smith, R.) Springer LNCS, Berlin, 
Germany, pp. 17-24.
Cholewiak, R. W. and Collins, A. (1996): Vibrotactile
pattern discrimination and communality at several 
body sites. Perception and Psychophysics,  57, 724-737
.
Craig, J. C. and Sherrick, C. E. (1982): Dynamic Tactile
Displays. In Tactual Perception: A Sourcebook (Ed, 
Foulke, E.) Cambridge University Press, pp. 209-233.
22
Edwards, A. D. N. (Ed.) (1995) Extra-Ordinary Human-Computer
Interaction, Cambridge University Press, 
Cambridge, UK.
Enriquez, M. J. and Maclean, K. (2003): The Hapticon
editor: A tool in support of haptic communication research
.  Haptics Symposium 2003, Los Angeles, CA, 
356-362, IEEE Press
Geldard, F. A. (1957): Adventures in tactile literacy. The
American Psychologist, 12, 115-124.
Gemperle, F., Kasabach, C., Stivoric, J., Bauer, M. and
Martin, R. (1998): Design for wearability. Proceedings
of Second International Symposium on Wearable 
Computers, Los Alamitos, CA, 116-122, IEEE Computer
Society
Gill, J. (2003), Vol. 2003 Royal National Institute of the
Blind, UK.
Gunther, E. (2001): Skinscape: A Tool for Composition in
the Tactile Modality. Massachusetts Institute of Technology
. Masters of Engineering.
Gunther, E., Davenport, G. and O'Modhrain, S. (2002):
Cutaneous Grooves: Composing for the Sense of 
Touch.  Proceedings of Conference on New Instruments
for Musical Expression, Dublin, IR, 1-6,
Jansson, G. and Larsson, K. (2002): Identification of
Haptic Virtual Objects with Differing Degrees of 
Complexity. Proceedings of Eurohaptics 2002, Edinburgh
, UK, 57-60, Edinburgh University
Kaczmarek, K., Webster, J., Bach-y-Rita, P. and Tomp-kins
, W. (1991): Electrotacile and vibrotactile displays
for sensory substitution systems. IEEE Transaction
on Biomedical Engineering, 38, 1-16.
Kurze, M. (1997): Rendering drawings for interactive
haptic perception. Proceedings of ACM CHI'97, Atlanta
, GA, 423-430, ACM Press, Addison-Wesley
Kurze, M. (1998): TGuide: a guidance system for tactile
image exploration. Proceedings of ACM ASSETS '98, 
Marina del Rey, CA, ACM Press
Lederman, S. J. and Klatzky, R. L. (1999): Sensing and
Displaying Spatially Distributed Fingertip Forces in 
Haptic Interfaces for Teleoperator and Virtual Environment
Systems. Presence: Teleoperators and Virtual
Environments, 8, 86-103.
Montagu, A. (1971): Touching: The Human Significance
of the Skin, Columbia University Press, New York.
Oakley, I., McGee, M., Brewster, S. A. and Gray, P. D.
(2000): Putting the feel in look and feel. Proceedings 
of ACM CHI 2000, The Hague, Netherlands, 415-422, 
ACM Press, Addison-Wesley
Rupert, A. (2000): Tactile situation awareness system:
proprioceptive prostheses for sensory deficiencies. 
Aviation, Space and Environmental Medicine, 71, 92-99
.
Sherrick, C. (1985): A scale for rate of tactual vibration.
Journal of the Acoustical Society of America, 78.
Shneiderman, B. (1998): Designing the user interface, 3
rd

Ed. Addison-Wesley, Reading (MA).
Spence, C. and Driver, J. (1997): Cross-modal links in
attention between audition, vision and touch: implications
for interface design. International Journal of 
Cognitive Ergonomics, 1, 351-373.
Stone, R. (2000): Haptic feedback: A potted history, from
telepresence to virtual reality. The First International 
Workshop on Haptic Human-Computer Interaction, 
Glasgow, UK, 1-7, Springer-Verlag Lecture Notes in 
Computer Science
Summers, I. R., Chanter, C. M., Southall, A. L. and
Brady, A. C. (2001): Results from a Tactile Array on 
the Fingertip. Proceedings of Eurohaptics 2001, Birmingham
, UK, 26-28, University of Birmingham
Tan, H. Z. and Pentland, A. (1997): Tactual Displays for
Wearable Computing. Proceedings of the First International
Symposium on Wearable Computers, IEEE
Tan, H. Z. and Pentland, A. (2001): Chapter 18: Tactual
displays for sensory substitution and wearable computers
. In Fundamentals of wearable computers and 
augmented reality (Eds, Barfield, W. and Caudell, T.) 
Lawrence Erlbaum Associates, Mahwah, New Jersey, 
pp. 579-598.
van Erp, J. B. F. (2002): Guidelines for the use of active
vibro-tactile displays in human-computer interaction. 
Proceedings of Eurohaptics 2002, Edinburgh, UK, 
18-22, University of Edinburgh
Van Scoy, F., Kawai, T., Darrah, M. and Rash, C. (2000):
Haptic Display of Mathematical Functions for Teaching
Mathematics to Students with Vision Disabilities: 
Design and Proof of Concept. Proceedings of the First 
Workshop on Haptic Human-Computer Interaction, 
Glasgow, UK, University of Glasgow
van Veen, H. and van Erp, J. B. F. (2001): Tactile information
presentation in the cockpit. In Haptic Human-Computer
Interaction (LNCS2058), Vol. 2058 (Eds, 
Brewster, S. A. and Murray-Smith, R.) Springer, Berlin
, Germany, pp. 174-181.
Wall, S. A. and Harwin, W. S. (2001): A High Bandwidth
Interface for Haptic Human Computer Interaction. 
Mechatronics. The Science of Intelligent Machines. 
An International Journal, 11, 371-387.
Wall, S. A., Riedel, B., Crossan, A. and McGee, M. R.
(Eds.) (2002) Eurohaptics 2002 Conference Proceedings
, University of Edinburgh, Edinburgh, Scotland.
Wies, E., Gardner, J., O'Modhrain, S., Hasser, C. and
Bulatov, V. (2001): Web-based touch display for accessible
science education. In Haptic Human-Computer
Interaction, Vol. 2058 (Eds, Brewster, S. 
A. and Murray-Smith, R.) Springer LNCS, Berlin, pp. 
52-60.
